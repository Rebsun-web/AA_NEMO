{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "from datetime import datetime, timedelta\n",
    "import json\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import urllib3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "# Manual download"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "we need the ASD station"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_traffic_file(file_path):\n",
    "    \"\"\"\n",
    "    Complete pipeline for processing traffic files\n",
    "    \"\"\"\n",
    "    # Read the CSV\n",
    "    df = pd.read_csv(file_path, on_bad_lines='skip')\n",
    "\n",
    "    numeric_columns = [\"duration_minutes\"]\n",
    "        \n",
    "    # Convert all numeric columns to float first, then handle NaN values\n",
    "    for col in numeric_columns:\n",
    "        if col in df.columns:\n",
    "            # Convert to string, remove any non-numeric characters except decimal point\n",
    "            df[col] = df[col].astype(str).str.replace(r'[^\\d.-]', '', regex=True)\n",
    "            # Convert to numeric, set errors to NaN\n",
    "            df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "            # Fill NaN with 0\n",
    "            df[col] = df[col].fillna(0)\n",
    "\n",
    "    df[\"start_time\"] = pd.to_datetime(df[\"start_time\"])\n",
    "    df[\"start_time_date\"] = df[\"start_time\"].dt.date\n",
    "    # df[\"start_time_date\"] = df[\"start_time\"].dt.strftime('%Y-%m-%d') \n",
    "        \n",
    "    # Select important columns for analysis\n",
    "    important_columns = [\n",
    "        \"rdt_station_codes\", \"cause_en\", \"cause_group\", \"start_time_date\", \"duration_minutes\"\n",
    "    ]\n",
    "    \n",
    "    # Filter to only include columns that exist in the dataframe\n",
    "    available_columns = [col for col in important_columns if col in df.columns]\n",
    "    \n",
    "    return df[available_columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "directory_path = \"../Data_Raw/Traffic/\"\n",
    "\n",
    "csv_files = [f for f in os.listdir(directory_path) if f.endswith('.csv')]\n",
    "\n",
    "combined_df = pd.DataFrame()\n",
    "for file in csv_files:\n",
    "    file_path = os.path.join(directory_path, file)\n",
    "    print(f\"Processing: {file}\")\n",
    "    df = process_traffic_file(file_path)\n",
    "    df['source_file'] = file\n",
    "    combined_df = pd.concat([combined_df, df], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df = combined_df[combined_df['rdt_station_codes'].str.contains('ASD', na=False)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_data_path = \"../../Data_Sources/Data_Cleaned/Traffic/\"\n",
    "\n",
    "# Create the target directory if it doesn't exist\n",
    "os.makedirs(cleaned_data_path, exist_ok=True)\n",
    "\n",
    "# Save the DataFrame to the target directory\n",
    "output_file = os.path.join(cleaned_data_path, \"disruptions_data_historical.csv\")\n",
    "filtered_df.to_csv(output_file, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {},
   "source": [
    "# Work with API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Disable SSL warnings for OVAPI\n",
    "# urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)\n",
    "\n",
    "# class TransportDisruptionCollector:\n",
    "#     def __init__(self, ns_api_key=None):\n",
    "#         # NS API configuration\n",
    "#         self.ns_api_key = ns_api_key\n",
    "#         if ns_api_key:\n",
    "#             self.ns_headers = {\n",
    "#                 'Ocp-Apim-Subscription-Key': self.ns_api_key,\n",
    "#                 'Accept': 'application/json'\n",
    "#             }\n",
    "        \n",
    "#         # OVAPI configuration - use HTTP to avoid SSL issues\n",
    "#         self.ovapi_base = \"http://v0.ovapi.nl\"\n",
    "        \n",
    "#         # Amsterdam Central Station TPC codes\n",
    "#         self.central_station_tpcs = [\"30009501\", \"30009500\"]\n",
    "        \n",
    "#         # Data storage\n",
    "#         self.disruptions_data = []\n",
    "        \n",
    "#     def get_ns_disruptions(self, start_date, end_date):\n",
    "#         \"\"\"\n",
    "#         Fetch disruptions from NS API for a date range.\n",
    "#         \"\"\"\n",
    "#         current_date = datetime.now()\n",
    "        \n",
    "#         if end_date.year <= 2024:\n",
    "#             # Use rijdendetreinen.nl open data for historical data\n",
    "#             print(\"Getting historical NS disruption data references...\")\n",
    "#             return self.get_historical_ns_references(start_date, end_date)\n",
    "#         else:\n",
    "#             # Use NS API for current/future data\n",
    "#             print(\"Fetching current NS disruption data from NS API...\")\n",
    "#             return self.get_current_ns_disruptions()\n",
    "    \n",
    "#     def get_historical_ns_references(self, start_date, end_date):\n",
    "#         \"\"\"\n",
    "#         Return references to historical NS disruption data files.\n",
    "#         \"\"\"\n",
    "#         historical_references = []\n",
    "        \n",
    "#         for year in range(start_date.year, end_date.year + 1):\n",
    "#             if year <= 2023:\n",
    "#                 url = f\"https://www.rijdendetreinen.nl/en/open-data/disruptions/{year}\"\n",
    "#                 historical_references.append({\n",
    "#                     'type': 'NS_HISTORICAL_REFERENCE',\n",
    "#                     'year': year,\n",
    "#                     'source': 'rijdendetreinen.nl',\n",
    "#                     'url': url,\n",
    "#                     'description': f\"Download CSV file for {year} disruptions\",\n",
    "#                     'note': 'Manual download required'\n",
    "#                 })\n",
    "#             else:\n",
    "#                 print(f\"Historical data for {year} not yet available\")\n",
    "        \n",
    "#         return historical_references\n",
    "    \n",
    "#     def load_ns_historical_csv(self, csv_path):\n",
    "#         \"\"\"\n",
    "#         Load NS historical disruption data from downloaded CSV file.\n",
    "#         \"\"\"\n",
    "#         try:\n",
    "#             df = pd.read_csv(csv_path)\n",
    "#             historical_data = []\n",
    "            \n",
    "#             for _, row in df.iterrows():\n",
    "#                 historical_data.append({\n",
    "#                     'type': 'NS_HISTORICAL',\n",
    "#                     'source': 'rijdendetreinen.nl',\n",
    "#                     'disruption_id': row.get('id'),\n",
    "#                     'start_time': row.get('start_time'),\n",
    "#                     'end_time': row.get('end_time'),\n",
    "#                     'station': row.get('stations'),\n",
    "#                     'cause': row.get('cause'),\n",
    "#                     'description': row.get('title'),\n",
    "#                     'statistical_cause': row.get('statistical_cause'),\n",
    "#                     'cause_group': row.get('cause_group')\n",
    "#                 })\n",
    "            \n",
    "#             return historical_data\n",
    "            \n",
    "#         except Exception as e:\n",
    "#             print(f\"Error loading historical CSV: {e}\")\n",
    "#             return []\n",
    "    \n",
    "#     def get_current_ns_disruptions(self):\n",
    "#         \"\"\"\n",
    "#         Fetch current NS disruptions from the API.\n",
    "#         \"\"\"\n",
    "#         if not self.ns_api_key:\n",
    "#             print(\"No NS API key provided. Cannot fetch current disruptions.\")\n",
    "#             return []\n",
    "            \n",
    "#         url = \"https://gateway.apiportal.ns.nl/reisinformatie-api/api/v3/disruptions\"\n",
    "        \n",
    "#         try:\n",
    "#             response = requests.get(url, headers=self.ns_headers)\n",
    "#             response.raise_for_status()\n",
    "#             data = response.json()\n",
    "            \n",
    "#             current_disruptions = []\n",
    "            \n",
    "#             # Check if data is a list or dictionary with 'payload'\n",
    "#             if isinstance(data, list):\n",
    "#                 disruptions_list = data\n",
    "#             elif isinstance(data, dict) and 'payload' in data:\n",
    "#                 disruptions_list = data.get('payload', [])\n",
    "#             else:\n",
    "#                 print(f\"Unexpected NS API response format\")\n",
    "#                 return []\n",
    "            \n",
    "#             for disruption in disruptions_list:\n",
    "#                 current_disruptions.append({\n",
    "#                     'type': 'NS_REALTIME',\n",
    "#                     'source': 'NS_API',\n",
    "#                     'disruption_id': disruption.get('id'),\n",
    "#                     'start_time': disruption.get('start'),\n",
    "#                     'end_time': disruption.get('end'),\n",
    "#                     'station': disruption.get('location', {}).get('name'),\n",
    "#                     'cause': disruption.get('cause'),\n",
    "#                     'description': disruption.get('title')\n",
    "#                 })\n",
    "            \n",
    "#             return current_disruptions\n",
    "            \n",
    "#         except requests.RequestException as e:\n",
    "#             print(f\"Error fetching NS disruptions: {e}\")\n",
    "#             return []\n",
    "    \n",
    "#     def get_gvb_disruptions(self):\n",
    "#         \"\"\"\n",
    "#         Fetch GVB (Amsterdam public transport) disruptions from OVAPI.\n",
    "#         \"\"\"\n",
    "#         disruptions = []\n",
    "        \n",
    "#         for stop_code in self.central_station_tpcs:\n",
    "#             url = f\"{self.ovapi_base}/tpc/{stop_code}\"\n",
    "            \n",
    "#             try:\n",
    "#                 response = requests.get(url)\n",
    "#                 response.raise_for_status()\n",
    "#                 data = response.json()\n",
    "                \n",
    "#                 # Navigate through the correct data structure\n",
    "#                 for area_code, area_data in data.items():\n",
    "#                     for tpc, tpc_data in area_data.items():\n",
    "#                         stop_info = tpc_data.get('Stop', {})\n",
    "                        \n",
    "#                         if 'Passes' in tpc_data:\n",
    "#                             for pass_id, pass_data in tpc_data['Passes'].items():\n",
    "#                                 # Check for disruptions in the pass data\n",
    "                                \n",
    "#                                 # Check if journey is cancelled\n",
    "#                                 if pass_data.get('TripStopStatus') == 'CANCEL':\n",
    "#                                     disruptions.append({\n",
    "#                                         'type': 'GVB',\n",
    "#                                         'source': 'OVAPI',\n",
    "#                                         'stop_code': stop_code,\n",
    "#                                         'stop_name': stop_info.get('TimingPointName', 'Amsterdam Centraal'),\n",
    "#                                         'line': pass_data.get('LinePublicNumber'),\n",
    "#                                         'status': 'CANCELLED',\n",
    "#                                         'expected_departure': pass_data.get('ExpectedDepartureTime'),\n",
    "#                                         'target_departure': pass_data.get('TargetDepartureTime'),\n",
    "#                                         'destination': pass_data.get('DestinationName50'),\n",
    "#                                         'timestamp': datetime.now().isoformat()\n",
    "#                                     })\n",
    "                                \n",
    "#                                 # Check for delays\n",
    "#                                 expected_time = pass_data.get('ExpectedDepartureTime')\n",
    "#                                 target_time = pass_data.get('TargetDepartureTime')\n",
    "                                \n",
    "#                                 if expected_time and target_time:\n",
    "#                                     try:\n",
    "#                                         expected_dt = datetime.fromisoformat(expected_time.replace('T', ' '))\n",
    "#                                         target_dt = datetime.fromisoformat(target_time.replace('T', ' '))\n",
    "#                                         delay_minutes = (expected_dt - target_dt).total_seconds() / 60\n",
    "                                        \n",
    "#                                         if delay_minutes > 5:  # Consider significant delays\n",
    "#                                             disruptions.append({\n",
    "#                                                 'type': 'GVB',\n",
    "#                                                 'source': 'OVAPI',\n",
    "#                                                 'stop_code': stop_code,\n",
    "#                                                 'stop_name': stop_info.get('TimingPointName', 'Amsterdam Centraal'),\n",
    "#                                                 'line': pass_data.get('LinePublicNumber'),\n",
    "#                                                 'status': 'DELAYED',\n",
    "#                                                 'delay_minutes': delay_minutes,\n",
    "#                                                 'expected_departure': expected_time,\n",
    "#                                                 'target_departure': target_time,\n",
    "#                                                 'destination': pass_data.get('DestinationName50'),\n",
    "#                                                 'timestamp': datetime.now().isoformat()\n",
    "#                                             })\n",
    "#                                     except ValueError:\n",
    "#                                         print(f\"Error parsing datetime for {pass_id}\")\n",
    "                        \n",
    "#                         # Check for general messages (disruptions)\n",
    "#                         if 'GeneralMessages' in tpc_data and tpc_data['GeneralMessages']:\n",
    "#                             for msg_id, message in tpc_data['GeneralMessages'].items():\n",
    "#                                 disruptions.append({\n",
    "#                                     'type': 'GVB',\n",
    "#                                     'source': 'OVAPI',\n",
    "#                                     'stop_code': stop_code,\n",
    "#                                     'stop_name': stop_info.get('TimingPointName', 'Amsterdam Centraal'),\n",
    "#                                     'line': None,\n",
    "#                                     'status': 'GENERAL_MESSAGE',\n",
    "#                                     'message': message,\n",
    "#                                     'timestamp': datetime.now().isoformat()\n",
    "#                                 })\n",
    "                \n",
    "#             except requests.RequestException as e:\n",
    "#                 print(f\"Error fetching GVB data for stop {stop_code}: {e}\")\n",
    "        \n",
    "#         return disruptions\n",
    "    \n",
    "#     def collect_current_data(self):\n",
    "#         \"\"\"\n",
    "#         Collect current disruption data from both NS and GVB.\n",
    "#         \"\"\"\n",
    "#         print(\"Collecting current data for transport disruptions...\")\n",
    "        \n",
    "#         # Collect NS disruptions\n",
    "#         ns_disruptions = self.get_current_ns_disruptions()\n",
    "#         self.disruptions_data.extend(ns_disruptions)\n",
    "        \n",
    "#         # Collect GVB disruptions\n",
    "#         gvb_disruptions = self.get_gvb_disruptions()\n",
    "#         self.disruptions_data.extend(gvb_disruptions)\n",
    "        \n",
    "#         print(f\"Collected {len(self.disruptions_data)} disruption records\")\n",
    "        \n",
    "#         return self.create_dataframe()\n",
    "    \n",
    "#     def collect_historical_data(self, start_date, end_date):\n",
    "#         \"\"\"\n",
    "#         Collect historical disruption data references.\n",
    "#         \"\"\"\n",
    "#         print(\"Collecting historical data references...\")\n",
    "        \n",
    "#         # Get NS historical references\n",
    "#         ns_references = self.get_historical_ns_references(start_date, end_date)\n",
    "#         self.disruptions_data.extend(ns_references)\n",
    "        \n",
    "#         print(f\"Collected {len(self.disruptions_data)} records\")\n",
    "        \n",
    "#         return self.create_dataframe()\n",
    "    \n",
    "#     def create_dataframe(self):\n",
    "#         \"\"\"\n",
    "#         Convert collected data to a pandas DataFrame with proper error handling.\n",
    "#         \"\"\"\n",
    "#         if not self.disruptions_data:\n",
    "#             return pd.DataFrame()\n",
    "        \n",
    "#         df = pd.DataFrame(self.disruptions_data)\n",
    "        \n",
    "#         # Rename columns for consistency\n",
    "#         column_mapping = {\n",
    "#             'start_time': 'start_datetime',\n",
    "#             'end_time': 'end_datetime',\n",
    "#             'expected_departure': 'start_datetime',\n",
    "#             'target_departure': 'planned_departure'\n",
    "#         }\n",
    "        \n",
    "#         for old_col, new_col in column_mapping.items():\n",
    "#             if old_col in df.columns and new_col not in df.columns:\n",
    "#                 df = df.rename(columns={old_col: new_col})\n",
    "        \n",
    "#         # Process datetime columns more carefully\n",
    "#         datetime_columns = ['start_datetime', 'end_datetime', 'planned_departure']\n",
    "        \n",
    "#         for col in datetime_columns:\n",
    "#             if col in df.columns:\n",
    "#                 # First replace None/NaN values\n",
    "#                 df[col] = df[col].fillna('')\n",
    "                \n",
    "#                 # Convert to string and handle special cases\n",
    "#                 df[col] = df[col].astype(str)\n",
    "#                 df[col] = df[col].replace(['None', 'nan', 'NaT', ''], '')\n",
    "                \n",
    "#                 # Try to convert to datetime\n",
    "#                 try:\n",
    "#                     df[col] = pd.to_datetime(df[col], errors='coerce')\n",
    "#                 except Exception as e:\n",
    "#                     print(f\"Error converting {col} to datetime: {e}\")\n",
    "#                     continue\n",
    "                \n",
    "#                 # Double-check if conversion was successful\n",
    "#                 if not pd.api.types.is_datetime64_any_dtype(df[col]):\n",
    "#                     print(f\"Warning: {col} is not datetime type after conversion\")\n",
    "#                     # Create empty columns for derived features\n",
    "#                     df['date'] = None\n",
    "#                     df['hour'] = None\n",
    "#                     df['day_of_week'] = None\n",
    "#                     df['day_name'] = None\n",
    "#                     continue\n",
    "        \n",
    "#         # Add derived columns only if we have valid datetime data\n",
    "#         if 'start_datetime' in df.columns:\n",
    "#             if pd.api.types.is_datetime64_any_dtype(df['start_datetime']) and df['start_datetime'].notna().any():\n",
    "#                 # Extract date components safely\n",
    "#                 try:\n",
    "#                     df['date'] = df['start_datetime'].dt.date\n",
    "#                     df['hour'] = df['start_datetime'].dt.hour\n",
    "#                     df['day_of_week'] = df['start_datetime'].dt.dayofweek\n",
    "#                     df['day_name'] = df['start_datetime'].dt.day_name()\n",
    "#                 except Exception as e:\n",
    "#                     print(f\"Error creating derived date columns: {e}\")\n",
    "#             else:\n",
    "#                 # Create empty columns if datetime conversion failed\n",
    "#                 df['date'] = None\n",
    "#                 df['hour'] = None\n",
    "#                 df['day_of_week'] = None\n",
    "#                 df['day_name'] = None\n",
    "        \n",
    "#         # Fill in missing values for key columns\n",
    "#         for col in ['status', 'cause', 'description']:\n",
    "#             if col in df.columns:\n",
    "#                 df[col] = df[col].fillna('Unknown')\n",
    "        \n",
    "#         return df\n",
    "    \n",
    "#     def save_disruptions(self, df, filepath):\n",
    "#         \"\"\"\n",
    "#         Save disruptions to CSV file.\n",
    "#         \"\"\"\n",
    "#         df.to_csv(filepath, index=False)\n",
    "#         print(f\"Data saved to {filepath}\")\n",
    "    \n",
    "#     def generate_summary_report(self, df):\n",
    "#         \"\"\"\n",
    "#         Generate a summary report of the disruptions.\n",
    "#         \"\"\"\n",
    "#         if len(df) == 0:\n",
    "#             print(\"No data to summarize\")\n",
    "#             return\n",
    "        \n",
    "#         print(\"\\n\" + \"=\"*50)\n",
    "#         print(\"TRANSPORT DISRUPTION SUMMARY REPORT\")\n",
    "#         print(\"=\"*50)\n",
    "        \n",
    "#         # Basic statistics\n",
    "#         print(f\"\\nTotal disruptions: {len(df)}\")\n",
    "        \n",
    "#         # By type\n",
    "#         if 'type' in df.columns:\n",
    "#             print(\"\\nDisruptions by type:\")\n",
    "#             type_counts = df['type'].value_counts()\n",
    "#             for dtype, count in type_counts.items():\n",
    "#                 print(f\"  {dtype}: {count}\")\n",
    "        \n",
    "#         # By status\n",
    "#         if 'status' in df.columns:\n",
    "#             print(\"\\nDisruptions by status:\")\n",
    "#             status_counts = df['status'].value_counts()\n",
    "#             for status, count in status_counts.items():\n",
    "#                 print(f\"  {status}: {count}\")\n",
    "        \n",
    "#         # For GVB disruptions, show line statistics\n",
    "#         gvb_data = df[df['type'] == 'GVB'] if 'type' in df.columns else pd.DataFrame()\n",
    "#         if not gvb_data.empty and 'line' in gvb_data.columns:\n",
    "#             print(\"\\nGVB disruptions by line:\")\n",
    "#             line_counts = gvb_data['line'].value_counts()\n",
    "#             for line, count in line_counts.items():\n",
    "#                 if pd.notna(line):\n",
    "#                     print(f\"  Line {line}: {count}\")\n",
    "        \n",
    "#         # Time analysis\n",
    "#         if 'date' in df.columns and df['date'].notna().any():\n",
    "#             print(\"\\nDisruptions by date:\")\n",
    "#             date_counts = df['date'].value_counts().sort_index()\n",
    "#             for date, count in date_counts.items():\n",
    "#                 print(f\"  {date}: {count}\")\n",
    "        \n",
    "#         if 'hour' in df.columns and df['hour'].notna().any():\n",
    "#             print(\"\\nDisruptions by hour of day:\")\n",
    "#             hour_counts = df['hour'].value_counts().sort_index()\n",
    "#             for hour, count in hour_counts.items():\n",
    "#                 print(f\"  {hour:02d}:00: {count}\")\n",
    "        \n",
    "#         if 'day_name' in df.columns and df['day_name'].notna().any():\n",
    "#             print(\"\\nDisruptions by day of week:\")\n",
    "#             day_counts = df['day_name'].value_counts()\n",
    "#             for day, count in day_counts.items():\n",
    "#                 print(f\"  {day}: {count}\")\n",
    "        \n",
    "#         print(\"\\n\" + \"=\"*50)\n",
    "\n",
    "# # Example usage\n",
    "# def main():\n",
    "#     \"\"\"\n",
    "#     Main function to demonstrate usage of the TransportDisruptionCollector\n",
    "#     \"\"\"\n",
    "#     # Initialize collector (without NS API key for demonstration)\n",
    "#     collector = TransportDisruptionCollector(ns_api_key=\"ae2e4c01521d42bb93487bb1d174673c\")\n",
    "    \n",
    "#     # Option 1: Collect current disruption data\n",
    "#     print(\"=== Collecting Current Disruptions ===\")\n",
    "#     current_df = collector.collect_current_data()\n",
    "    \n",
    "#     # Generate summary report\n",
    "#     collector.generate_summary_report(current_df)\n",
    "    \n",
    "#     # Save to CSV\n",
    "#     current_filepath = \"nemo_transport_disruptions_current.csv\"\n",
    "#     collector.save_disruptions(current_df, current_filepath)\n",
    "    \n",
    "#     # Option 2: Get historical data references\n",
    "#     print(\"\\n=== Getting Historical Data References ===\")\n",
    "#     start_date = datetime(2022, 1, 1)\n",
    "#     end_date = datetime(2023, 12, 31)\n",
    "    \n",
    "#     historical_df = collector.collect_historical_data(start_date, end_date)\n",
    "    \n",
    "#     # Generate summary for historical references\n",
    "#     collector.generate_summary_report(historical_df)\n",
    "    \n",
    "#     # Save historical references\n",
    "#     historical_filepath = \"nemo_transport_disruptions_references.csv\"\n",
    "#     collector.save_disruptions(historical_df, historical_filepath)\n",
    "    \n",
    "#     # Option 3: Load a specific historical CSV (example)\n",
    "#     # Uncomment and modify path when you have downloaded historical data\n",
    "#     # historical_data = collector.load_ns_historical_csv(\"path/to/downloaded_file.csv\")\n",
    "#     # historical_df_loaded = pd.DataFrame(historical_data)\n",
    "#     # collector.save_disruptions(historical_df_loaded, \"nemo_historical_disruptions_2022.csv\")\n",
    "    \n",
    "#     return current_df, historical_df\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     current_data, historical_data = main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
