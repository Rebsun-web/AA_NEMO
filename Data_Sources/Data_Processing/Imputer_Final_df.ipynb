{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from datetime import datetime, timedelta\n",
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "# Merge DataSets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"../Data_Raw/Data_Cleaned/\"\n",
    "datasets = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "for file in os.listdir(path=path):\n",
    "    file_name = file.split('.')[0]\n",
    "    file_path = os.path.join(path, file)\n",
    "\n",
    "    encodings = ['utf-8', 'latin1', 'ISO-8859-1', 'cp1252']\n",
    "    success = False\n",
    "    \n",
    "    for encoding in encodings:\n",
    "        try:\n",
    "            if file.endswith('.csv'):\n",
    "                df = pd.read_csv(file_path, encoding=encoding)\n",
    "            elif file.endswith(('.xlsx', '.xls')):\n",
    "                df = pd.read_excel(file_path)\n",
    "            else:\n",
    "                # Skip non-CSV/Excel files\n",
    "                print(f\"Skipping unsupported file format: {file}\")\n",
    "                break\n",
    "                \n",
    "            datasets[file_name] = df\n",
    "            print(f\"Successfully loaded {file_name} with {encoding} encoding\")\n",
    "            success = True\n",
    "            break\n",
    "            \n",
    "        except UnicodeDecodeError:\n",
    "            print(f\"Failed to decode {file} with {encoding}, trying next encoding...\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading {file}: {str(e)}\")\n",
    "            break\n",
    "    \n",
    "    if not success and file.endswith('.csv'):\n",
    "        print(f\"Could not load {file} with any of the attempted encodings.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pivot Hotel table\n",
    "temp_df = datasets[\"hotel_updated\"]\n",
    "pivot_df = temp_df.pivot(index='Datum', columns='Region', values='Value (x1000)')\n",
    "pivot_df = pivot_df.reset_index()\n",
    "datasets[\"hotel_updated\"] = pivot_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_date = \"2022-01-26\"\n",
    "end_date = \"2025-04-13\"\n",
    "\n",
    "column_rename = {\n",
    "    \"Daily_Sentiment_Summary\" : {\"Publicatiedatum\" : \"Date\"},\n",
    "    \"Disruptions_Combined_With_After_Jan2023\" : {\"date\" : \"Date\"},\n",
    "    \"forecast_data_cleaned\" : {\"date\" : \"Date\"},\n",
    "    \"hotel_updated\" : {\"Datum\" : \"Date\"},\n",
    "    \"Amsterdam Events\" : {\"date\" : \"Date\", \"event_count\" : \"Events_in_Ams\"},\n",
    "    \"disruptions_data_historical\" : {\"start_time_date\" : \"Date\", \"duration_minutes_sum\" : \"duration_minutes\", \"disruptions_count\" : \"disruptions_count\"}\n",
    "}\n",
    "\n",
    "needed_columns = {\n",
    "    \"forecast_data_cleaned\" : [\"date\", \"is_open\", \"school_holiday\", \"public_holiday\", \"maat_visitors\"],\n",
    "    \"disruptions_data_historical\" : [\"start_time_date\", \"duration_minutes_sum\", \"disruptions_count\"]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Refactor dfs\n",
    "for file_name, df in datasets.items():\n",
    "    temp_df = df.copy()\n",
    "    \n",
    "    if file_name in needed_columns:\n",
    "        needed_col = needed_columns[file_name]\n",
    "        temp_df = temp_df[needed_col]\n",
    "    \n",
    "    if file_name in column_rename:\n",
    "        temp_df = temp_df.rename(columns=column_rename[file_name])\n",
    "    \n",
    "    datasets[file_name] = temp_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_merged_dataset(datasets, start_date, end_date):\n",
    "    start = pd.to_datetime(start_date)\n",
    "    end = pd.to_datetime(end_date)\n",
    "    \n",
    "    date_range = pd.date_range(start=start, end=end, freq='D')\n",
    "    merged_df = pd.DataFrame({'Date': date_range})\n",
    "    merged_df['Date'] = pd.to_datetime(merged_df['Date'])\n",
    "    \n",
    "    for file_name, df in datasets.items():\n",
    "        print(f\"Processing {file_name}...\")\n",
    "\n",
    "        if file_name == \"Daily_Sentiment_Summary\":\n",
    "            continue\n",
    "        \n",
    "        temp_df = df.copy()\n",
    "        \n",
    "        if 'Date' not in temp_df.columns:\n",
    "            print(f\"Warning: No 'Date' column in {file_name}, skipping...\")\n",
    "            continue\n",
    "        \n",
    "        temp_df['Date'] = pd.to_datetime(temp_df['Date'])\n",
    "        \n",
    "        if temp_df['Date'].duplicated().any():\n",
    "            print(f\"Warning: Found duplicate dates in {file_name}, keeping first occurrence...\")\n",
    "            temp_df = temp_df.drop_duplicates(subset=['Date'], keep='first')\n",
    "        \n",
    "        cols_to_merge = [col for col in temp_df.columns if col != 'Date']\n",
    "        \n",
    "        # Skip if no columns left after removing Date\n",
    "        if not cols_to_merge:\n",
    "            print(f\"Warning: No data columns in {file_name} after removing Date, skipping...\")\n",
    "            continue\n",
    "        \n",
    "        # temp_df = temp_df.rename(columns={col: f\"{file_name}_{col}\" for col in cols_to_merge})\n",
    "        \n",
    "        # Add Date column back\n",
    "        # cols_to_merge = [f\"{file_name}_{col}\" for col in cols_to_merge]\n",
    "        \n",
    "        # Left merge with our main dataframe\n",
    "        merged_df = merged_df.merge(\n",
    "            temp_df[['Date'] + cols_to_merge], \n",
    "            on='Date', \n",
    "            how='left'\n",
    "        )\n",
    "        \n",
    "        print(f\"Added {len(cols_to_merge)} columns from {file_name}\")\n",
    "    \n",
    "    print(f\"Final merged dataset has {len(merged_df)} rows and {len(merged_df.columns)} columns\")\n",
    "    return merged_df\n",
    "\n",
    "merged_dataset = create_merged_dataset(datasets, start_date, end_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_nan_and_zeros(df):\n",
    "    # Create a DataFrame to store the results\n",
    "    result = pd.DataFrame(index=df.columns, columns=['NaN_Count', 'Zero_Count'])\n",
    "    \n",
    "    # Count NaN values in each column\n",
    "    result['NaN_Count'] = df.isna().sum()\n",
    "    \n",
    "    # Count zeros in each column\n",
    "    for column in df.columns:\n",
    "        # Need to handle different data types appropriately\n",
    "        if pd.api.types.is_numeric_dtype(df[column]):\n",
    "            # For numeric columns, count exact zeros\n",
    "            result.loc[column, 'Zero_Count'] = (df[column] == 0).sum()\n",
    "        elif pd.api.types.is_string_dtype(df[column]):\n",
    "            # For string columns, count empty strings and '0' strings\n",
    "            result.loc[column, 'Zero_Count'] = ((df[column] == '') | (df[column] == '0')).sum()\n",
    "        else:\n",
    "            # For other types, try to count zeros but set to NaN if not applicable\n",
    "            try:\n",
    "                result.loc[column, 'Zero_Count'] = (df[column] == 0).sum()\n",
    "            except:\n",
    "                result.loc[column, 'Zero_Count'] = np.nan\n",
    "    \n",
    "    # Calculate percentage of dataset\n",
    "    total_rows = len(df)\n",
    "    result['NaN_Percentage'] = (result['NaN_Count'] / total_rows * 100).round(2)\n",
    "    # result['Zero_Percentage'] = (result['Zero_Count'] / total_rows * 100).round(2)\n",
    "    \n",
    "    return result\n",
    "\n",
    "count_nan_and_zeros(merged_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {},
   "source": [
    "# Handle NaN-values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {},
   "source": [
    "### Statuses imputation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "def impute_is_open(df):\n",
    "    result_df = df.copy()\n",
    "    \n",
    "    # Add weekday information (0=Monday, 6=Sunday)\n",
    "    result_df['weekday'] = pd.to_datetime(result_df['Date']).dt.dayofweek\n",
    "    \n",
    "    # Rule 1: If there were visitors, the museum was open\n",
    "    mask_visitors = result_df['Total_Visitors'] > 0\n",
    "    result_df.loc[mask_visitors, 'is_open'] = 1\n",
    "    \n",
    "    # Rule 2: If there were no visitors, the museum was likely closed\n",
    "    mask_no_visitors = result_df['Total_Visitors'] == 0\n",
    "    result_df.loc[mask_no_visitors, 'is_open'] = 0\n",
    "    \n",
    "    # Rule 3: For remaining NaN values, use typical opening pattern\n",
    "    weekday_pattern = result_df.groupby('weekday')['is_open'].agg(\n",
    "        lambda x: x.mode()[0] if not x.mode().empty else 1\n",
    "    )\n",
    "    \n",
    "    # Apply weekday pattern to remaining NaN values\n",
    "    for weekday, typical_status in weekday_pattern.items():\n",
    "        mask_weekday = (result_df['weekday'] == weekday) & (result_df['is_open'].isna())\n",
    "        result_df.loc[mask_weekday, 'is_open'] = typical_status\n",
    "    \n",
    "    return result_df\n",
    "\n",
    "\n",
    "def impute_school_holiday(df):\n",
    "    result_df = df.copy()\n",
    "    \n",
    "    # Rule 1: If we have school groups (PO or VO > 0), it's not a school holiday\n",
    "    mask_school_groups = (result_df['PO'] > 0) | (result_df['VO'] > 0)\n",
    "    result_df.loc[mask_school_groups, 'school_holiday'] = 0\n",
    "    \n",
    "    # Rule 2: Use holiday_nl as reference for remaining NaN values\n",
    "    result_df.loc[result_df['school_holiday'].isna(), 'school_holiday'] = \\\n",
    "        result_df.loc[result_df['school_holiday'].isna(), 'holiday_nl']\n",
    "    \n",
    "    return result_df\n",
    "\n",
    "\n",
    "def impute_public_holiday(df):\n",
    "    result_df = df.copy()\n",
    "    \n",
    "    # Rule 1: Use holiday_all as base (if any country has a holiday)\n",
    "    result_df.loc[result_df['public_holiday'].isna(), 'public_holiday'] = \\\n",
    "        result_df.loc[result_df['public_holiday'].isna(), 'holiday_all']\n",
    "    \n",
    "    # Rule 2: If museum is closed and it's not a weekend, likely a public holiday\n",
    "    weekend_mask = result_df['weekday'].isin([5, 6])  # Saturday and Sunday\n",
    "    closed_weekday_mask = (result_df['is_open'] == 0) & (~weekend_mask)\n",
    "    result_df.loc[closed_weekday_mask, 'public_holiday'] = 1\n",
    "    \n",
    "    return result_df\n",
    "\n",
    "\n",
    "# Add this to your notebook\n",
    "def impute_all_status(df):\n",
    "    result_df = df.copy()\n",
    "    \n",
    "    # First impute is_open as it's used by other imputations\n",
    "    result_df = impute_is_open(result_df)\n",
    "    \n",
    "    # Then impute holidays\n",
    "    result_df = impute_school_holiday(result_df)\n",
    "    result_df = impute_public_holiday(result_df)\n",
    "    \n",
    "    # Drop the temporary weekday column if it exists\n",
    "    if 'weekday' in result_df.columns:\n",
    "        result_df = result_df.drop('weekday', axis=1)\n",
    "    \n",
    "    return result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_dataset = impute_all_status(merged_dataset)\n",
    "\n",
    "# Verify results\n",
    "print(\"\\nMissing values after imputation:\")\n",
    "print(merged_dataset[['is_open', 'school_holiday', 'public_holiday']].isna().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13",
   "metadata": {},
   "source": [
    "### Traffic disruption imputation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Impute duration_minutes using linear regression\n",
    "def impute_disruption_duration(df, column_to_impute):\n",
    "    result_df = df.copy()\n",
    "    \n",
    "    # Add weekday feature\n",
    "    result_df['weekday'] = pd.to_datetime(result_df['Date']).dt.dayofweek\n",
    "    \n",
    "    # Select features for the model\n",
    "    features = [\n",
    "        'MeanTemp_C', \n",
    "        'Precipitation_mm', \n",
    "        'Sunshine_hours', \n",
    "        'weekday',\n",
    "        'Events_in_Ams',\n",
    "        'public_holiday',\n",
    "        'school_holiday',\n",
    "        'North Holland (PV)',\n",
    "        'South Holland (PV)',\n",
    "        'Utrecht (PV)'\n",
    "    ]\n",
    "    \n",
    "    # Split into training and prediction sets\n",
    "    train_mask = ~result_df[column_to_impute].isna()\n",
    "    X_train = result_df.loc[train_mask, features]\n",
    "    y_train = result_df.loc[train_mask, column_to_impute]\n",
    "    X_pred = result_df.loc[~train_mask, features]\n",
    "    \n",
    "    # Train model and make predictions\n",
    "    model = LinearRegression()\n",
    "    model.fit(X_train, y_train)\n",
    "    predictions = model.predict(X_pred)\n",
    "    \n",
    "    # Ensure non-negative durations\n",
    "    predictions = np.maximum(predictions, 0)\n",
    "    \n",
    "    # Fill in predictions\n",
    "    result_df.loc[~train_mask, column_to_impute] = predictions\n",
    "    \n",
    "    # Clean up temporary columns\n",
    "    result_df = result_df.drop('weekday', axis=1)\n",
    "    \n",
    "    return result_df\n",
    "\n",
    "# Apply the imputation\n",
    "merged_dataset = impute_disruption_duration(merged_dataset, 'duration_minutes')\n",
    "merged_dataset = impute_disruption_duration(merged_dataset, 'disruptions_count')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15",
   "metadata": {},
   "source": [
    "### Add Seasonal Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime, timedelta\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"../Data_Raw/Data_Seasonal_Patterns/\"\n",
    "datasets = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "for file in os.listdir(path=path):\n",
    "    file_name = file.split('.')[0]\n",
    "    file_path = os.path.join(path, file)\n",
    "\n",
    "    encodings = ['utf-8', 'latin1', 'ISO-8859-1', 'cp1252']\n",
    "    \n",
    "    for encoding in encodings:\n",
    "        if file.endswith('.csv'):\n",
    "            df = pd.read_csv(file_path, encoding=encoding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class AmsterdamTourismFeatureEngine:\n",
    "    def __init__(self):\n",
    "        self.monthly_data = {}\n",
    "        self.feature_mappings = {}\n",
    "        \n",
    "    def load_monthly_data(self, path):\n",
    "        \"\"\"Load all monthly CSV files\"\"\"\n",
    "\n",
    "        datasets = {\n",
    "            'hotels': 'Hotels.csv',\n",
    "            'theaters': 'Theaters.csv', \n",
    "            'nemo': 'Nemo.csv',\n",
    "            'corporate': 'Corporate.csv',\n",
    "            'museums': 'Museums.csv'\n",
    "        }\n",
    "\n",
    "        for file in os.listdir(path=path):\n",
    "            file_name = file.split('.')[0]\n",
    "            file_path = os.path.join(path, file)\n",
    "\n",
    "            key = file_name.lower().replace(' ', '_')\n",
    "\n",
    "            encodings = ['utf-8', 'latin1', 'ISO-8859-1', 'cp1252']\n",
    "    \n",
    "            for encoding in encodings:\n",
    "                if file.endswith('.csv'):\n",
    "                    df = pd.read_csv(file_path,sep=',', encoding=encoding)\n",
    "                    break\n",
    "\n",
    "            df_clean = self._clean_monthly_data(df, key)\n",
    "            self.monthly_data[key] = df_clean\n",
    "\n",
    "\n",
    "    def _clean_monthly_data(self, df, dataset_type):\n",
    "        \"\"\"Clean and standardize monthly data format\"\"\"\n",
    "        # Handle different column structures\n",
    "        month_cols = ['Jan', 'Feb', 'Mrt', 'Apr', 'Mei', 'Jun', 'Jul', 'Aug', 'Sep', 'Okt', 'Nov', 'Dec']\n",
    "        \n",
    "        # Extract years from first column or infer from row count\n",
    "        years = []\n",
    "        monthly_values = []\n",
    "        \n",
    "        for idx, row in df.iterrows():\n",
    "            if idx == 0:  # Skip header if needed\n",
    "                continue\n",
    "            \n",
    "            year_str = str(row.iloc[0])\n",
    "            if any(char.isdigit() for char in year_str):\n",
    "                year = int(''.join(filter(str.isdigit, year_str)))\n",
    "                if 2020 <= year <= 2025:\n",
    "                    years.append(year)\n",
    "                    \n",
    "                    # Extract monthly values\n",
    "                    month_vals = []\n",
    "                    for col in month_cols:\n",
    "                        if col in df.columns:\n",
    "                            val = row[col]\n",
    "                            # Clean numeric values\n",
    "                            if pd.notna(val) and val != '':\n",
    "                                try:\n",
    "                                    val = float(str(val).replace(',', '').replace(' ', ''))\n",
    "                                    month_vals.append(val)\n",
    "                                except:\n",
    "                                    month_vals.append(0)\n",
    "                            else:\n",
    "                                month_vals.append(0)\n",
    "                    monthly_values.append(month_vals)\n",
    "        \n",
    "        # Create clean DataFrame\n",
    "        df_clean = pd.DataFrame(monthly_values, columns=month_cols, index=years)\n",
    "        return df_clean\n",
    "\n",
    "    \n",
    "    def create_tourism_features(self, target_dates):\n",
    "        \"\"\"Create all tourism-based features for given dates\"\"\"\n",
    "        # Convert dates to datetime if they're strings\n",
    "        if isinstance(target_dates[0], str):\n",
    "            target_dates = pd.to_datetime(target_dates)\n",
    "            \n",
    "        features_df = pd.DataFrame(index=target_dates)\n",
    "        \n",
    "        # 1. Tourism Intensity Indicators\n",
    "        features_df = self._add_tourism_intensity_features(features_df)\n",
    "        \n",
    "        # 2. Cultural Activity Features  \n",
    "        features_df = self._add_cultural_features(features_df)\n",
    "        \n",
    "        # 3. Monthly Pattern Features\n",
    "        features_df = self._add_seasonal_features(features_df)\n",
    "        \n",
    "        # 4. Trend & Growth Features\n",
    "        features_df = self._add_trend_features(features_df)\n",
    "        \n",
    "        # 5. Cross-Sector Correlation Features\n",
    "        features_df = self._add_correlation_features(features_df)\n",
    "        \n",
    "        # 6. Day-Level Application Features\n",
    "        features_df = self._add_daily_distribution_features(features_df)\n",
    "        \n",
    "        return features_df\n",
    "    \n",
    "\n",
    "    def _add_tourism_intensity_features(self, df):\n",
    "        \"\"\"Tourism intensity indicators\"\"\"\n",
    "        for date in df.index:\n",
    "            year, month = date.year, date.month\n",
    "            month_name = ['Jan', 'Feb', 'Mrt', 'Apr', 'Mei', 'Jun', 'Jul', 'Aug', 'Sep', 'Okt', 'Nov', 'Dec'][month-1]\n",
    "            \n",
    "            # Hotel occupancy index (normalized by annual average)\n",
    "            if 'hotels' in self.monthly_data:\n",
    "                hotel_data = self.monthly_data['hotels']\n",
    "                \n",
    "                if year in hotel_data.index:\n",
    "                    # Use exact year data if available\n",
    "                    monthly_hotels = hotel_data.loc[year, month_name]\n",
    "                    annual_avg = hotel_data.loc[year].mean()\n",
    "                else:\n",
    "                    # Use average across all years for this month\n",
    "                    monthly_hotels = hotel_data[month_name].mean()\n",
    "                    # For annual average, use the overall mean across all years/months\n",
    "                    annual_avg = hotel_data.values.mean()\n",
    "                    print(f\"Using average {month_name} data: {monthly_hotels:.2f} (overall avg: {annual_avg:.2f})\")\n",
    "                \n",
    "                df.loc[date, 'hotel_occupancy_index'] = monthly_hotels / annual_avg if annual_avg > 0 else 1\n",
    "                \n",
    "                # Tourism season strength (percentile ranking using multi-year data)\n",
    "                all_monthly_values = hotel_data[month_name].values  # All years for this month\n",
    "                percentile = (np.sum(all_monthly_values < monthly_hotels) / len(all_monthly_values)) * 100\n",
    "                df.loc[date, 'tourism_season_strength'] = percentile\n",
    "            else:\n",
    "                # No hotel data available - use defaults\n",
    "                df.loc[date, 'hotel_occupancy_index'] = 1.0\n",
    "                df.loc[date, 'tourism_season_strength'] = 50.0\n",
    "                    \n",
    "            # International visitor ratio (corporate meetings proxy)\n",
    "            if 'corporate' in self.monthly_data:\n",
    "                corp_data = self.monthly_data['corporate']\n",
    "                if year in corp_data.index and 'hotels' in self.monthly_data:\n",
    "                    monthly_corp = corp_data.loc[year, month_name]\n",
    "                    monthly_hotels = self.monthly_data['hotels'].loc[year, month_name]\n",
    "                    df.loc[date, 'international_visitor_ratio'] = (\n",
    "                        monthly_corp / monthly_hotels if monthly_hotels > 0 else 0\n",
    "                    )\n",
    "        \n",
    "        return df\n",
    "    \n",
    "\n",
    "    def _add_cultural_features(self, df):\n",
    "        \"\"\"Cultural activity features\"\"\"\n",
    "        for date in df.index:\n",
    "            year, month = date.year, date.month\n",
    "            month_name = ['Jan', 'Feb', 'Mrt', 'Apr', 'Mei', 'Jun', 'Jul', 'Aug', 'Sep', 'Okt', 'Nov', 'Dec'][month-1]\n",
    "            \n",
    "            # Cultural engagement score\n",
    "            cultural_total = 0\n",
    "            if 'theaters' in self.monthly_data and year in self.monthly_data['theaters'].index:\n",
    "                cultural_total += self.monthly_data['theaters'].loc[year, month_name]\n",
    "            if 'museums' in self.monthly_data and year in self.monthly_data['museums'].index:\n",
    "                cultural_total += self.monthly_data['museums'].loc[year, month_name]\n",
    "                \n",
    "            df.loc[date, 'cultural_engagement_score'] = cultural_total\n",
    "            \n",
    "            # Cultural vs tourism ratio\n",
    "            if 'hotels' in self.monthly_data and year in self.monthly_data['hotels'].index:\n",
    "                hotel_visits = self.monthly_data['hotels'].loc[year, month_name]\n",
    "                df.loc[date, 'cultural_vs_tourism_ratio'] = (\n",
    "                    cultural_total / hotel_visits if hotel_visits > 0 else 0\n",
    "                )\n",
    "            \n",
    "            # NEMO market share\n",
    "            if ('nemo' in self.monthly_data and 'museums' in self.monthly_data and \n",
    "                year in self.monthly_data['nemo'].index and \n",
    "                year in self.monthly_data['museums'].index):\n",
    "                nemo_visits = self.monthly_data['nemo'].loc[year, month_name]\n",
    "                total_museums = self.monthly_data['museums'].loc[year, month_name]\n",
    "                df.loc[date, 'nemo_market_share'] = (\n",
    "                    nemo_visits / total_museums if total_museums > 0 else 0\n",
    "                )\n",
    "        \n",
    "        return df\n",
    "    \n",
    "\n",
    "    def _add_seasonal_features(self, df):\n",
    "        \"\"\"Monthly pattern features\"\"\"\n",
    "        # Calculate overall tourism activity for ranking\n",
    "        if 'hotels' in self.monthly_data:\n",
    "            hotel_data = self.monthly_data['hotels']\n",
    "            \n",
    "            # Monthly rankings across all years\n",
    "            all_monthly_avgs = {}\n",
    "            months = ['Jan', 'Feb', 'Mrt', 'Apr', 'Mei', 'Jun', 'Jul', 'Aug', 'Sep', 'Okt', 'Nov', 'Dec']\n",
    "            \n",
    "            for i, month_name in enumerate(months):\n",
    "                month_vals = []\n",
    "                for year in hotel_data.index:\n",
    "                    month_vals.append(hotel_data.loc[year, month_name])\n",
    "                all_monthly_avgs[i+1] = np.mean(month_vals)\n",
    "            \n",
    "            # Rank months by average activity\n",
    "            month_rankings = {month: rank for rank, month in enumerate(\n",
    "                sorted(all_monthly_avgs.keys(), key=lambda x: all_monthly_avgs[x], reverse=True), 1\n",
    "            )}\n",
    "            \n",
    "            for date in df.index:\n",
    "                month = date.month\n",
    "                year = date.year\n",
    "                month_name = months[month-1]\n",
    "                \n",
    "                # Month tourism rank (1 = highest tourism month)\n",
    "                df.loc[date, 'month_tourism_rank'] = month_rankings[month]\n",
    "                \n",
    "                # Seasonal multiplier\n",
    "                if year in hotel_data.index:\n",
    "                    monthly_val = hotel_data.loc[year, month_name]\n",
    "                    annual_avg = hotel_data.loc[year].mean()\n",
    "                    df.loc[date, 'seasonal_multiplier'] = monthly_val / annual_avg if annual_avg > 0 else 1\n",
    "                \n",
    "                # Peak season flag (top 4 months)\n",
    "                df.loc[date, 'peak_season_flag'] = 1 if month_rankings[month] <= 4 else 0\n",
    "                \n",
    "                # Shoulder season flag (middle 4 months)\n",
    "                df.loc[date, 'shoulder_season_flag'] = 1 if 5 <= month_rankings[month] <= 8 else 0\n",
    "        \n",
    "        return df\n",
    "    \n",
    "\n",
    "    def _add_trend_features(self, df):\n",
    "        \"\"\"Trend and growth features\"\"\"\n",
    "        for dataset_name in self.monthly_data.keys():\n",
    "            data = self.monthly_data[dataset_name]\n",
    "            \n",
    "            for date in df.index:\n",
    "                year, month = date.year, date.month\n",
    "                month_name = ['Jan', 'Feb', 'Mrt', 'Apr', 'Mei', 'Jun', 'Jul', 'Aug', 'Sep', 'Okt', 'Nov', 'Dec'][month-1]\n",
    "                \n",
    "                # Year-over-year growth\n",
    "                if year in data.index and (year-1) in data.index:\n",
    "                    current_val = data.loc[year, month_name]\n",
    "                    prev_year_val = data.loc[year-1, month_name]\n",
    "                    yoy_growth = ((current_val - prev_year_val) / prev_year_val * 100 if prev_year_val > 0 else 0)\n",
    "                    df.loc[date, f'{dataset_name}_yoy_growth'] = yoy_growth\n",
    "                \n",
    "                # 3-month momentum (if we have previous months)\n",
    "                if year in data.index:\n",
    "                    months = ['Jan', 'Feb', 'Mrt', 'Apr', 'Mei', 'Jun', 'Jul', 'Aug', 'Sep', 'Okt', 'Nov', 'Dec']\n",
    "                    current_idx = months.index(month_name)\n",
    "                    \n",
    "                    if current_idx >= 2:  # Need at least 3 months\n",
    "                        recent_vals = [data.loc[year, months[current_idx-i]] for i in range(3)]\n",
    "                        momentum = (recent_vals[0] - recent_vals[2]) / recent_vals[2] * 100 if recent_vals[2] > 0 else 0\n",
    "                        df.loc[date, f'{dataset_name}_momentum'] = momentum\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    def _add_correlation_features(self, df):\n",
    "        \"\"\"Cross-sector correlation features\"\"\"\n",
    "        for date in df.index:\n",
    "            year, month = date.year, date.month\n",
    "            month_name = ['Jan', 'Feb', 'Mrt', 'Apr', 'Mei', 'Jun', 'Jul', 'Aug', 'Sep', 'Okt', 'Nov', 'Dec'][month-1]\n",
    "            \n",
    "            # Business vs leisure balance\n",
    "            if ('corporate' in self.monthly_data and 'hotels' in self.monthly_data and\n",
    "                year in self.monthly_data['corporate'].index and \n",
    "                year in self.monthly_data['hotels'].index):\n",
    "                \n",
    "                corp_val = self.monthly_data['corporate'].loc[year, month_name]\n",
    "                hotel_val = self.monthly_data['hotels'].loc[year, month_name]\n",
    "                leisure_proxy = hotel_val - corp_val  # Rough estimate\n",
    "                \n",
    "                df.loc[date, 'business_leisure_balance'] = (\n",
    "                    corp_val / leisure_proxy if leisure_proxy > 0 else 0\n",
    "                )\n",
    "            \n",
    "            # Competitor activity index (other museums vs NEMO)\n",
    "            if ('museums' in self.monthly_data and 'nemo' in self.monthly_data and\n",
    "                year in self.monthly_data['museums'].index and \n",
    "                year in self.monthly_data['nemo'].index):\n",
    "                \n",
    "                total_museums = self.monthly_data['museums'].loc[year, month_name]\n",
    "                nemo_visits = self.monthly_data['nemo'].loc[year, month_name]\n",
    "                other_museums = total_museums - nemo_visits\n",
    "                \n",
    "                df.loc[date, 'competitor_activity_index'] = (\n",
    "                    other_museums / nemo_visits if nemo_visits > 0 else 0\n",
    "                )\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    def _add_daily_distribution_features(self, df):\n",
    "        \"\"\"Day-level application features\"\"\"\n",
    "        # Standard daily distribution patterns (weekday/weekend effects)\n",
    "        weekend_boost = 1.2  # Museums typically see 20% boost on weekends\n",
    "        weekday_factors = [0.9, 0.95, 1.0, 1.05, 1.1, 1.25, 1.15]  # Mon-Sun\n",
    "        \n",
    "        for date in df.index:\n",
    "            year, month = date.year, date.month\n",
    "            month_name = ['Jan', 'Feb', 'Mrt', 'Apr', 'Mei', 'Jun', 'Jul', 'Aug', 'Sep', 'Okt', 'Nov', 'Dec'][month-1]\n",
    "            \n",
    "            # Monthly expected baseline (distributed daily)\n",
    "            if 'nemo' in self.monthly_data and year in self.monthly_data['nemo'].index:\n",
    "                monthly_total = self.monthly_data['nemo'].loc[year, month_name]\n",
    "                days_in_month = pd.Timestamp(year, month, 1).daysinmonth\n",
    "                \n",
    "                # Simple daily distribution\n",
    "                daily_baseline = monthly_total / days_in_month\n",
    "                \n",
    "                # Apply weekday factor\n",
    "                weekday_factor = weekday_factors[date.weekday()]\n",
    "                df.loc[date, 'monthly_expected_baseline'] = daily_baseline * weekday_factor\n",
    "            \n",
    "            # Tourism pressure coefficient\n",
    "            if 'hotel_occupancy_index' in df.columns:\n",
    "                tourism_pressure = df.loc[date, 'hotel_occupancy_index']\n",
    "                # Higher tourism = more competition for attention\n",
    "                df.loc[date, 'tourism_pressure_coefficient'] = max(0.5, min(1.5, tourism_pressure))\n",
    "            \n",
    "            # Cultural saturation factor\n",
    "            if 'competitor_activity_index' in df.columns:\n",
    "                competition = df.loc[date, 'competitor_activity_index']\n",
    "                # Higher competition = lower relative appeal\n",
    "                saturation_factor = 1 / (1 + competition * 0.1)  # Diminishing returns\n",
    "                df.loc[date, 'cultural_saturation_factor'] = saturation_factor\n",
    "        \n",
    "        return df\n",
    "\n",
    "# Usage example:\n",
    "def create_tourism_features_for_dates(date_range, path):\n",
    "    \"\"\"\n",
    "    Main function to create features for a date range\n",
    "    \n",
    "    Parameters:\n",
    "    date_range: list of dates or pandas date range\n",
    "    data_file_paths: dict mapping dataset names to file paths\n",
    "    \n",
    "    Returns:\n",
    "    DataFrame with tourism features\n",
    "    \"\"\"\n",
    "    \n",
    "    # Initialize feature engine\n",
    "    engine = AmsterdamTourismFeatureEngine()\n",
    "    \n",
    "    # Load data\n",
    "    engine.load_monthly_data(path)\n",
    "    \n",
    "    # Create features\n",
    "    features = engine.create_tourism_features(date_range)\n",
    "    \n",
    "    # Fill any remaining NaN values\n",
    "    features = features.fillna(method='ffill').fillna(0)\n",
    "    \n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "date_range = pd.date_range(start_date, end_date, freq='D')\n",
    "tourism_features = create_tourism_features_for_dates(date_range, path).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "tourism_features = tourism_features.rename(columns={'index': 'Date'}, inplace=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22",
   "metadata": {},
   "source": [
    "# Merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {},
   "outputs": [],
   "source": [
    "big_merged_df = merged_dataset.merge(\n",
    "            tourism_features, \n",
    "            on='Date', \n",
    "            how='left'\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {},
   "outputs": [],
   "source": [
    "modelling_path = \"../../Data_Sources/Data_Cleaned/Modelling/Table_for_modelling.csv\"\n",
    "big_merged_df.to_csv(modelling_path, index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
