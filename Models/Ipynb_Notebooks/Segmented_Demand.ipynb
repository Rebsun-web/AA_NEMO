{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from xgboost import XGBRegressor, callback\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import (\n",
    "    mean_squared_error,\n",
    "    r2_score, \n",
    "    mean_absolute_error\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read data\n",
    "file_path = \"../Data_Sources/Data_Cleaned/Table_for_modelling\"\n",
    "df = pd.read_csv(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visitors demand prediction by segments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "copy_df = df.copy()\n",
    "copy_df = copy_df.drop(columns=[\"maat_visitors\"]) # drop crew predictions, will be predicted later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SegmentedVisitorPredictor:\n",
    "    def __init__(self):\n",
    "        self.models = {}\n",
    "        self.scalers = {}\n",
    "        self.feature_sets = {}\n",
    "        \n",
    "        # Define segment mappings (original column names to standardized names)\n",
    "        self.segment_mappings = {\n",
    "            'Recreatief NL': 'recreatief_nl',\n",
    "            'Recreatief Buitenland': 'recreatief_buitenland',\n",
    "            'PO': 'po',\n",
    "            'VO': 'vo',\n",
    "            'Student': 'student',\n",
    "            'Extern': 'extern',\n",
    "            'Total Visitors': 'total_visitors'\n",
    "        }\n",
    "        \n",
    "        # Inverse mapping for converting back\n",
    "        self.inverse_segment_mappings = {\n",
    "            v: k for k, v in self.segment_mappings.items()\n",
    "        }\n",
    "\n",
    "    def standardize_column_names(self, df):\n",
    "        \"\"\"Standardize column names to snake_case\"\"\"\n",
    "        df = df.copy()\n",
    "        \n",
    "        # Rename segment columns\n",
    "        rename_dict = {\n",
    "            old: new for old, new in self.segment_mappings.items() \n",
    "            if old in df.columns\n",
    "        }\n",
    "        df = df.rename(columns=rename_dict)\n",
    "        \n",
    "        # Standardize other column names\n",
    "        df.columns = [\n",
    "            col.lower().replace(' ', '_').replace('/', '_') \n",
    "            for col in df.columns\n",
    "        ]\n",
    "        \n",
    "        return df\n",
    "\n",
    "    def engineer_features(self, df, target_segment=None):\n",
    "        \"\"\"\n",
    "        Engineer features with standardized column names.\n",
    "        If target_segment is provided, excludes current segment values\n",
    "        but keeps historical data.\n",
    "        \"\"\"\n",
    "        df = df.copy()\n",
    "\n",
    "        # Calculate total visitors if not present\n",
    "        if 'total_visitors' not in df.columns:\n",
    "            visitor_cols = [col for col in self.segment_mappings.values() if col != 'total_visitors' and col in df.columns]\n",
    "            df['total_visitors'] = df[visitor_cols].sum(axis=1)\n",
    "        \n",
    "        # Get all segment columns except the target\n",
    "        segment_cols = list(self.segment_mappings.values())\n",
    "        if target_segment and target_segment in df.columns:\n",
    "            if target_segment != \"total_visitors\":\n",
    "                # Only remove current values of other segments\n",
    "                current_segments = [col for col in segment_cols if col != target_segment and col != \"total_visitors\"]\n",
    "                df = df.drop(columns=current_segments)\n",
    "            else:\n",
    "                current_segments = [col for col in segment_cols if col != target_segment]\n",
    "                df = df.drop(columns=current_segments)\n",
    "\n",
    "        \n",
    "        # Convert date to datetime if not already\n",
    "        if 'date' in df.columns:\n",
    "            df['date'] = pd.to_datetime(df['date'])\n",
    "            \n",
    "            # Basic time features\n",
    "            df['year'] = df['date'].dt.year\n",
    "            df['month'] = df['date'].dt.month\n",
    "            df['day_of_week'] = df['date'].dt.dayofweek\n",
    "            df['is_weekend'] = df['day_of_week'].isin([5, 6]).astype(int)\n",
    "            \n",
    "            # Season (as one-hot encoded features)\n",
    "            season_mapping = {\n",
    "                12: 'winter', 1: 'winter', 2: 'winter',\n",
    "                3: 'spring', 4: 'spring', 5: 'spring',\n",
    "                6: 'summer', 7: 'summer', 8: 'summer',\n",
    "                9: 'fall', 10: 'fall', 11: 'fall'\n",
    "            }\n",
    "            df['season'] = df['date'].dt.month.map(season_mapping)\n",
    "            season_dummies = pd.get_dummies(\n",
    "                df['season'], \n",
    "                prefix='season',\n",
    "                drop_first=True  # Drop one category to avoid multicollinearity\n",
    "            )\n",
    "            df = pd.concat([df, season_dummies], axis=1)\n",
    "            df = df.drop('season', axis=1)\n",
    "        \n",
    "        # Weather features if available\n",
    "        weather_cols = ['meantemp_c', 'precipitation_mm']\n",
    "        if all(col in df.columns for col in weather_cols):\n",
    "            df['good_weather'] = (\n",
    "                (df['meantemp_c'] > 15) & \n",
    "                (df['precipitation_mm'] < 1)\n",
    "            ).astype(int)\n",
    "            df['bad_weather'] = (\n",
    "                (df['meantemp_c'] < 10) | \n",
    "                (df['precipitation_mm'] > 5)\n",
    "            ).astype(int)\n",
    "            \n",
    "            # Bin temperature into categories and one-hot encode\n",
    "            df['temp_category'] = pd.cut(\n",
    "                df['meantemp_c'],\n",
    "                bins=[-float('inf'), 5, 15, 25, float('inf')],\n",
    "                labels=['cold', 'mild', 'warm', 'hot']\n",
    "            )\n",
    "            temp_dummies = pd.get_dummies(\n",
    "                df['temp_category'], \n",
    "                prefix='temp',\n",
    "                drop_first=True\n",
    "            )\n",
    "            df = pd.concat([df, temp_dummies], axis=1)\n",
    "            df = df.drop('temp_category', axis=1)\n",
    "            \n",
    "            # Bin precipitation into categories and one-hot encode\n",
    "            df['precip_category'] = pd.cut(\n",
    "                df['precipitation_mm'],\n",
    "                bins=[-float('inf'), 0.1, 5, float('inf')],\n",
    "                labels=['dry', 'light', 'heavy']\n",
    "            )\n",
    "            precip_dummies = pd.get_dummies(\n",
    "                df['precip_category'], \n",
    "                prefix='precip',\n",
    "                drop_first=True\n",
    "            )\n",
    "            df = pd.concat([df, precip_dummies], axis=1)\n",
    "            df = df.drop('precip_category', axis=1)\n",
    "        \n",
    "        # Create day type features\n",
    "        df['is_monday'] = (df['day_of_week'] == 0).astype(int)\n",
    "        df['is_friday'] = (df['day_of_week'] == 4).astype(int)\n",
    "        df['is_saturday'] = (df['day_of_week'] == 5).astype(int)\n",
    "        df['is_sunday'] = (df['day_of_week'] == 6).astype(int)\n",
    "        \n",
    "        # Drop original day_of_week as we have more specific features now\n",
    "        df = df.drop('day_of_week', axis=1)\n",
    "        \n",
    "        # Add holiday interaction features if available\n",
    "        holiday_cols = [col for col in df.columns if 'holiday' in col.lower()]\n",
    "        if holiday_cols:\n",
    "            df['total_holidays'] = df[holiday_cols].sum(axis=1)\n",
    "            if target_segment == 'recreatief_nl':\n",
    "                df['nl_holiday_effect'] = df['holiday_nl'].fillna(0)\n",
    "            elif target_segment == 'recreatief_buitenland':\n",
    "                # For international visitors, consider international holidays\n",
    "                intl_holidays = [col for col in holiday_cols if col != 'holiday_nl']\n",
    "                df['intl_holiday_effect'] = df[intl_holidays].sum(axis=1)\n",
    "            elif target_segment in ['po', 'vo', 'student']:\n",
    "                # For educational segments, focus on relevant holidays\n",
    "                df['edu_holiday_effect'] = (\n",
    "                    df['holiday_nl'].fillna(0) * 2 +  # Double weight for local holidays\n",
    "                    df['total_holidays']\n",
    "                )\n",
    "        \n",
    "        return df\n",
    "\n",
    "    def add_lagged_features(self, df, segment, lags=[1, 7, 14, 28]):\n",
    "        \"\"\"Add lagged features using standardized column names\"\"\"\n",
    "        df = df.copy()\n",
    "        \n",
    "        # Get all segment columns as they represent historical data\n",
    "        segment_cols = [col for col in self.segment_mappings.values() \n",
    "                       if col in df.columns]\n",
    "        \n",
    "        for lag in lags:\n",
    "            # Add lags for all segments as they represent historical data\n",
    "            for seg in segment_cols:\n",
    "                df[f'{seg}_lag_{lag}'] = df[seg].shift(lag)\n",
    "            \n",
    "            # Add cross-segment features using historical data\n",
    "            if segment in ['recreatief_nl', 'recreatief_buitenland']:\n",
    "                rec_cols = ['recreatief_nl', 'recreatief_buitenland']\n",
    "                rec_cols = [col for col in rec_cols if col in df.columns]\n",
    "                if rec_cols:\n",
    "                    df[f'total_recreational_lag_{lag}'] = (\n",
    "                        df[rec_cols].fillna(0).sum(axis=1).shift(lag)\n",
    "                    )\n",
    "            \n",
    "            elif segment in ['po', 'vo', 'student']:\n",
    "                edu_cols = ['po', 'vo', 'student']\n",
    "                edu_cols = [col for col in edu_cols if col in df.columns]\n",
    "                if edu_cols:\n",
    "                    df[f'total_educational_lag_{lag}'] = (\n",
    "                        df[edu_cols].fillna(0).sum(axis=1).shift(lag)\n",
    "                    )\n",
    "            \n",
    "            # Add day-of-week specific lags for the target segment\n",
    "            if segment in df.columns:\n",
    "                # Last week same day\n",
    "                df[f'{segment}_lastweek_sameday'] = df[segment].shift(7)\n",
    "                # Average of last 4 same weekdays\n",
    "                df[f'{segment}_avg_4weeks_sameday'] = (\n",
    "                    df[segment].shift(7) + \n",
    "                    df[segment].shift(14) + \n",
    "                    df[segment].shift(21) + \n",
    "                    df[segment].shift(28)\n",
    "                ) / 4\n",
    "        \n",
    "        return df\n",
    "\n",
    "    def add_rolling_features(self, df, segment, windows=[7, 14, 30]):\n",
    "        \"\"\"Add rolling features using standardized column names\"\"\"\n",
    "        df = df.copy()\n",
    "        \n",
    "        # Get all segment columns as they represent historical data\n",
    "        segment_cols = [col for col in self.segment_mappings.values() \n",
    "                       if col in df.columns]\n",
    "        \n",
    "        for window in windows:\n",
    "            # Add rolling stats for all segments\n",
    "            for seg in segment_cols:\n",
    "                df[f'{seg}_rolling_mean_{window}'] = (\n",
    "                    df[seg].shift(1).rolling(window=window).mean()\n",
    "                )\n",
    "                df[f'{seg}_rolling_std_{window}'] = (\n",
    "                    df[seg].shift(1).rolling(window=window).std()\n",
    "                )\n",
    "            \n",
    "            # Add segment group rolling features\n",
    "            if segment in ['recreatief_nl', 'recreatief_buitenland']:\n",
    "                rec_cols = ['recreatief_nl', 'recreatief_buitenland']\n",
    "                rec_cols = [col for col in rec_cols if col in df.columns]\n",
    "                if rec_cols:\n",
    "                    df[f'total_recreational_rolling_{window}'] = (\n",
    "                        df[rec_cols].fillna(0).sum(axis=1)\n",
    "                        .shift(1).rolling(window=window).mean()\n",
    "                    )\n",
    "            \n",
    "            elif segment in ['po', 'vo', 'student']:\n",
    "                edu_cols = ['po', 'vo', 'student']\n",
    "                edu_cols = [col for col in edu_cols if col in df.columns]\n",
    "                if edu_cols:\n",
    "                    df[f'total_educational_rolling_{window}'] = (\n",
    "                        df[edu_cols].fillna(0).sum(axis=1)\n",
    "                        .shift(1).rolling(window=window).mean()\n",
    "                    )\n",
    "            \n",
    "            # Add holiday density if available\n",
    "            if 'is_holiday' in df.columns:\n",
    "                df[f'holiday_density_{window}'] = (\n",
    "                    df['is_holiday'].rolling(window=window).mean()\n",
    "                )\n",
    "        \n",
    "        return df\n",
    "\n",
    "    def prepare_segment_data(self, df, segment):\n",
    "        \"\"\"Prepare data for a specific segment using standardized column names\"\"\"\n",
    "        # Standardize column names\n",
    "        df_processed = self.standardize_column_names(df)\n",
    "        \n",
    "        # Engineer basic features without other segments\n",
    "        df_processed = self.engineer_features(df_processed, target_segment=segment)\n",
    "        \n",
    "        # Add lagged and rolling features\n",
    "        df_processed = self.add_lagged_features(df_processed, segment)\n",
    "        df_processed = self.add_rolling_features(df_processed, segment)\n",
    "        \n",
    "        # Drop rows with NaN values (usually at the start due to lagging)\n",
    "        df_processed = df_processed.dropna()\n",
    "        \n",
    "        # Drop date column as it's not needed for modeling\n",
    "        if 'date' in df_processed.columns:\n",
    "            df_processed = df_processed.drop('date', axis=1)\n",
    "        \n",
    "        # Select features and target\n",
    "        features = [\n",
    "            col for col in df_processed.columns \n",
    "            if col != segment\n",
    "        ]\n",
    "        print(f\"Features used for {segment}: {features}\")\n",
    "\n",
    "        X = df_processed[features]\n",
    "        y = df_processed[segment]\n",
    "        \n",
    "        return X, y, features\n",
    "\n",
    "    def train_segment_model(self, X, y, segment, visitor_predictions):\n",
    "        \"\"\"Train model for a specific segment\"\"\"\n",
    "        \n",
    "        # Split data\n",
    "        X_train, X_test, y_train, y_test = train_test_split(\n",
    "            X, y, test_size=0.2, random_state=42\n",
    "        )\n",
    "        \n",
    "        # Scale features\n",
    "        scaler = StandardScaler()\n",
    "        # Convert to numpy arrays for scaling\n",
    "        X_train_scaled = scaler.fit_transform(X_train.to_numpy())\n",
    "        X_test_scaled = scaler.transform(X_test.to_numpy())\n",
    "        \n",
    "        # Initialize and train model with segment-specific parameters\n",
    "        if segment == 'total_visitors':\n",
    "            # For total visitors, use a more robust model\n",
    "            model = XGBRegressor(\n",
    "                n_estimators=1000,\n",
    "                learning_rate=0.01,\n",
    "                max_depth=7,\n",
    "                subsample=0.8,\n",
    "                colsample_bytree=0.8,\n",
    "                min_child_weight=3,\n",
    "                random_state=42,\n",
    "                eval_metric=['rmse', 'mae']\n",
    "            )\n",
    "        elif segment == 'extern':\n",
    "            # For extern segment (poorest performing), use more complex model\n",
    "            model = XGBRegressor(\n",
    "                n_estimators=1000,\n",
    "                learning_rate=0.005,\n",
    "                max_depth=8,\n",
    "                subsample=0.8,\n",
    "                colsample_bytree=0.7,\n",
    "                min_child_weight=3,\n",
    "                random_state=42,\n",
    "                eval_metric=['rmse', 'mae']\n",
    "            )\n",
    "        elif segment in ['vo', 'student']:\n",
    "            # For VO and Student segments (moderate performance)\n",
    "            model = XGBRegressor(\n",
    "                n_estimators=750,\n",
    "                learning_rate=0.008,\n",
    "                max_depth=6,\n",
    "                subsample=0.85,\n",
    "                colsample_bytree=0.8,\n",
    "                min_child_weight=2,\n",
    "                random_state=42,\n",
    "                eval_metric=['rmse', 'mae']\n",
    "            )\n",
    "        else:\n",
    "            # For better performing segments\n",
    "            model = XGBRegressor(\n",
    "                n_estimators=500,\n",
    "                learning_rate=0.01,\n",
    "                max_depth=5,\n",
    "                subsample=0.8,\n",
    "                colsample_bytree=0.8,\n",
    "                random_state=42,\n",
    "                eval_metric=['rmse', 'mae']\n",
    "            )\n",
    "        \n",
    "        \n",
    "        # Train the model\n",
    "        model.fit(\n",
    "            X_train_scaled, \n",
    "            y_train,\n",
    "            eval_set=[(X_train_scaled, y_train), (X_test_scaled, y_test)],\n",
    "            verbose=False\n",
    "        )\n",
    "                \n",
    "        # Evaluate\n",
    "        y_pred_test = model.predict(X_test_scaled)\n",
    "        y_pred_train = model.predict(X_train_scaled)\n",
    "        \n",
    "        # Store predictions with indices\n",
    "        visitor_predictions[segment] = {\n",
    "            'train_predictions': pd.Series(y_pred_train, index=X_train.index),\n",
    "            'test_predictions': pd.Series(y_pred_test, index=X_test.index),\n",
    "            'train_actual': y_train,\n",
    "            'test_actual': y_test\n",
    "        }\n",
    "\n",
    "        test_rmse = np.sqrt(mean_squared_error(y_test, y_pred_test))\n",
    "        test_r2 = r2_score(y_test, y_pred_test)\n",
    "        test_mae = mean_absolute_error(y_test, y_pred_test)\n",
    "\n",
    "        train_rmse = np.sqrt(mean_squared_error(y_train, y_pred_train))\n",
    "        train_r2 = r2_score(y_train, y_pred_train)\n",
    "        train_mae = mean_absolute_error(y_train, y_pred_train)\n",
    "        \n",
    "        print(f\"\\nResults for {self.inverse_segment_mappings[segment]}:\")\n",
    "        print(f\"Test_RMSE: {test_rmse:.4f}\")\n",
    "        print(f\"Test_R²: {test_r2:.4f}\")\n",
    "        print(f\"Test_MAE: {test_mae:.4f}\")\n",
    "\n",
    "        print(f\"Train_RMSE: {train_rmse:.4f}\")\n",
    "        print(f\"Train_R²: {train_r2:.4f}\")\n",
    "        print(f\"Train_MAE: {train_mae:.4f}\")\n",
    "        \n",
    "        # Print feature importance\n",
    "        feature_importance = pd.DataFrame({\n",
    "            'feature': X.columns,\n",
    "            'importance': model.feature_importances_\n",
    "        })\n",
    "        feature_importance = feature_importance.sort_values(\n",
    "            'importance', ascending=False\n",
    "        ).head(10)\n",
    "        \n",
    "        print(\"\\nTop 10 Most Important Features:\")\n",
    "        for _, row in feature_importance.iterrows():\n",
    "            print(f\"{row['feature']}: {row['importance']:.4f}\")\n",
    "        \n",
    "        return model, scaler, visitor_predictions\n",
    "\n",
    "    def fit(self, df):\n",
    "        \"\"\"Fit models for all segments\"\"\"\n",
    "\n",
    "        # Dictionary to save visitor predictions by segments\n",
    "        visitor_predictions = {}\n",
    "\n",
    "        for original_segment, standardized_segment in self.segment_mappings.items():\n",
    "            print(f\"\\nTraining model for {original_segment}\")\n",
    "            print(\"=\"*50)\n",
    "            \n",
    "            # Prepare data\n",
    "            X, y, features = self.prepare_segment_data(df, standardized_segment)\n",
    "            \n",
    "            # Train model\n",
    "            model, scaler, visitor_predictions = self.train_segment_model(X, y, standardized_segment, visitor_predictions)\n",
    "            \n",
    "            # Store model, scaler and features\n",
    "            self.models[standardized_segment] = model\n",
    "            self.scalers[standardized_segment] = scaler\n",
    "            self.feature_sets[standardized_segment] = features\n",
    "\n",
    "    def predict(self, df):\n",
    "        \"\"\"Make predictions for all segments\"\"\"\n",
    "        predictions = {}\n",
    "        \n",
    "        # Standardize input data column names\n",
    "        df = self.standardize_column_names(df)\n",
    "        \n",
    "        for original_segment, standardized_segment in self.segment_mappings.items():\n",
    "            if standardized_segment in self.models:\n",
    "                # Prepare features\n",
    "                df_processed = self.engineer_features(df, target_segment=standardized_segment)\n",
    "                df_processed = self.add_lagged_features(df_processed, standardized_segment)\n",
    "                df_processed = self.add_rolling_features(df_processed, standardized_segment)\n",
    "                \n",
    "                # Select features\n",
    "                features = self.feature_sets[standardized_segment]\n",
    "                X = df_processed[features]\n",
    "                \n",
    "                # Scale features\n",
    "                X_scaled = self.scalers[standardized_segment].transform(X)\n",
    "                \n",
    "                # Make predictions\n",
    "                predictions[original_segment] = self.models[standardized_segment].predict(X_scaled)\n",
    "        \n",
    "        return predictions\n",
    "\n",
    "\n",
    "    def get_predictions_dataframe(self, df):\n",
    "        \"\"\"\n",
    "        Create a DataFrame with predictions for all segments, preserving all original rows\n",
    "        \"\"\"\n",
    "        # First, fit the models if not already done\n",
    "        if not self.models:\n",
    "            self.fit(df)\n",
    "        \n",
    "        # Initialize results with full original index\n",
    "        all_predictions = {}\n",
    "        original_index = df.index\n",
    "        \n",
    "        # Initialize all columns with NaN\n",
    "        for original_segment in self.segment_mappings.keys():\n",
    "            all_predictions[f'{original_segment}_pred'] = pd.Series(index=original_index, dtype=float)\n",
    "            all_predictions[f'{original_segment}_actual'] = pd.Series(index=original_index, dtype=float)\n",
    "            all_predictions[f'{original_segment}_set'] = pd.Series(index=original_index, dtype=str)\n",
    "        \n",
    "        # Get predictions from each model for rows that can be processed\n",
    "        for segment in self.segment_mappings.values():\n",
    "            if segment in self.models:\n",
    "                # Re-prepare data to get the same train/test split\n",
    "                X, y, features = self.prepare_segment_data(df, segment)\n",
    "                X_train, X_test, y_train, y_test = train_test_split(\n",
    "                    X, y, test_size=0.2, random_state=42\n",
    "                )\n",
    "                \n",
    "                # Get predictions\n",
    "                X_train_scaled = self.scalers[segment].transform(X_train.to_numpy())\n",
    "                X_test_scaled = self.scalers[segment].transform(X_test.to_numpy())\n",
    "                \n",
    "                y_pred_train = self.models[segment].predict(X_train_scaled)\n",
    "                y_pred_test = self.models[segment].predict(X_test_scaled)\n",
    "                \n",
    "                original_name = self.inverse_segment_mappings[segment]\n",
    "                \n",
    "                # Fill in train predictions and labels\n",
    "                all_predictions[f'{original_name}_pred'].loc[X_train.index] = y_pred_train\n",
    "                all_predictions[f'{original_name}_actual'].loc[X_train.index] = y_train.values\n",
    "                all_predictions[f'{original_name}_set'].loc[X_train.index] = 'train'\n",
    "                \n",
    "                # Fill in test predictions and labels  \n",
    "                all_predictions[f'{original_name}_pred'].loc[X_test.index] = y_pred_test\n",
    "                all_predictions[f'{original_name}_actual'].loc[X_test.index] = y_test.values\n",
    "                all_predictions[f'{original_name}_set'].loc[X_test.index] = 'test'\n",
    "        \n",
    "        # Create DataFrame\n",
    "        predictions_df = pd.DataFrame(all_predictions)\n",
    "        \n",
    "        # Add date column\n",
    "        if 'Date' in df.columns:\n",
    "            predictions_df['Date'] = df['Date']\n",
    "        elif 'date' in df.columns:\n",
    "            predictions_df['Date'] = df['date']\n",
    "        \n",
    "        return predictions_df\n",
    "\n",
    "\n",
    "    def analyze_feature_importance(self, segment=None, top_n=15, plot=True):\n",
    "        \"\"\"\n",
    "        Analyze feature importance for a specific segment or all segments.\n",
    "        \n",
    "        Args:\n",
    "            segment (str, optional): Specific segment to analyze. \n",
    "                                   If None, analyzes all segments.\n",
    "            top_n (int): Number of top features to show.\n",
    "            plot (bool): Whether to create visualizations.\n",
    "            \n",
    "        Returns:\n",
    "            dict: Dictionary containing feature importance analysis for each segment.\n",
    "        \"\"\"\n",
    "        results = {}\n",
    "        segments_to_analyze = (\n",
    "            [segment] if segment \n",
    "            else list(self.segment_mappings.values())\n",
    "        )\n",
    "        \n",
    "        for seg in segments_to_analyze:\n",
    "            if seg not in self.models:\n",
    "                continue\n",
    "                \n",
    "            # Get feature importance\n",
    "            importance = self.models[seg].feature_importances_\n",
    "            features = self.feature_sets[seg]\n",
    "            \n",
    "            # Create importance DataFrame\n",
    "            imp_df = pd.DataFrame({\n",
    "                'feature': features,\n",
    "                'importance': importance\n",
    "            })\n",
    "            imp_df = imp_df.sort_values('importance', ascending=False)\n",
    "            \n",
    "            # Group features by type\n",
    "            feature_types = {\n",
    "                'lag': 'Historical Values',\n",
    "                'rolling': 'Rolling Statistics',\n",
    "                'season': 'Seasonal',\n",
    "                'temp': 'Temperature',\n",
    "                'precip': 'Precipitation',\n",
    "                'holiday': 'Holidays',\n",
    "                'is_': 'Day Type'\n",
    "            }\n",
    "            \n",
    "            def get_feature_type(feature_name):\n",
    "                for key, value in feature_types.items():\n",
    "                    if key in feature_name:\n",
    "                        return value\n",
    "                return 'Other'\n",
    "            \n",
    "            imp_df['feature_type'] = imp_df['feature'].apply(get_feature_type)\n",
    "            \n",
    "            # Calculate type importance\n",
    "            type_importance = (\n",
    "                imp_df.groupby('feature_type')['importance']\n",
    "                .sum()\n",
    "                .sort_values(ascending=False)\n",
    "            )\n",
    "            \n",
    "            # Store results\n",
    "            results[self.inverse_segment_mappings[seg]] = {\n",
    "                'top_features': imp_df.head(top_n),\n",
    "                'feature_type_importance': type_importance\n",
    "            }\n",
    "            \n",
    "            if plot:\n",
    "                # Create figure with two subplots\n",
    "                fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "                fig.suptitle(\n",
    "                    f'Feature Importance Analysis - {self.inverse_segment_mappings[seg]}',\n",
    "                    fontsize=14\n",
    "                )\n",
    "                \n",
    "                # Plot top features\n",
    "                sns.barplot(\n",
    "                    data=imp_df.head(top_n),\n",
    "                    x='importance',\n",
    "                    y='feature',\n",
    "                    ax=ax1\n",
    "                )\n",
    "                ax1.set_title('Top Individual Features')\n",
    "                ax1.set_xlabel('Importance Score')\n",
    "                \n",
    "                # Plot feature type importance\n",
    "                sns.barplot(\n",
    "                    x=type_importance.values,\n",
    "                    y=type_importance.index,\n",
    "                    ax=ax2\n",
    "                )\n",
    "                ax2.set_title('Feature Type Importance')\n",
    "                ax2.set_xlabel('Total Importance Score')\n",
    "                \n",
    "                plt.tight_layout()\n",
    "                plt.show()\n",
    "        \n",
    "        return results\n",
    "\n",
    "    def get_segment_insights(self, segment):\n",
    "        \"\"\"\n",
    "        Get detailed insights about what influences a specific segment.\n",
    "        \n",
    "        Args:\n",
    "            segment (str): The segment to analyze\n",
    "            \n",
    "        Returns:\n",
    "            dict: Dictionary containing insights about the segment\n",
    "        \"\"\"\n",
    "        if segment not in self.models:\n",
    "            return None\n",
    "            \n",
    "        # Get standardized segment name\n",
    "        std_segment = segment\n",
    "        if segment in self.inverse_segment_mappings:\n",
    "            std_segment = segment\n",
    "        elif segment in self.segment_mappings:\n",
    "            std_segment = self.segment_mappings[segment]\n",
    "        else:\n",
    "            return None\n",
    "            \n",
    "        # Get feature importance analysis\n",
    "        analysis = self.analyze_feature_importance(\n",
    "            segment=std_segment, \n",
    "            plot=False\n",
    "        )\n",
    "        segment_analysis = analysis[self.inverse_segment_mappings[std_segment]]\n",
    "        \n",
    "        # Extract key insights\n",
    "        top_features = segment_analysis['top_features']\n",
    "        type_importance = segment_analysis['feature_type_importance']\n",
    "        \n",
    "        # Generate insights\n",
    "        insights = {\n",
    "            'segment': self.inverse_segment_mappings[std_segment],\n",
    "            'top_5_features': top_features.head().to_dict('records'),\n",
    "            'main_drivers': type_importance.head(3).to_dict(),\n",
    "            'recommendations': []\n",
    "        }\n",
    "        \n",
    "        # Add specific recommendations based on feature importance\n",
    "        if 'Historical Values' in type_importance.head(3):\n",
    "            insights['recommendations'].append(\n",
    "                \"Strong dependence on historical patterns - \"\n",
    "                \"consider recent trends for predictions\"\n",
    "            )\n",
    "            \n",
    "        if 'Seasonal' in type_importance.head(3):\n",
    "            insights['recommendations'].append(\n",
    "                \"Seasonal factors are important - \"\n",
    "                \"plan for seasonal variations\"\n",
    "            )\n",
    "            \n",
    "        if 'Holidays' in type_importance.head(3):\n",
    "            insights['recommendations'].append(\n",
    "                \"Holiday periods significantly impact visitors - \"\n",
    "                \"adjust staffing during holidays\"\n",
    "            )\n",
    "            \n",
    "        if 'Temperature' in type_importance.head(3):\n",
    "            insights['recommendations'].append(\n",
    "                \"Weather sensitive segment - \"\n",
    "                \"consider weather forecasts in planning\"\n",
    "            )\n",
    "        \n",
    "        return insights "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    predictor = SegmentedVisitorPredictor()\n",
    "        \n",
    "    # Train on your data\n",
    "    predictor.fit(copy_df)\n",
    "\n",
    "    # Make predictions\n",
    "    predictions = predictor.predict(copy_df)\n",
    "\n",
    "    # Get Predictions DataFrame\n",
    "    predictions_df = predictor.get_predictions_dataframe(copy_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The most important features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor.analyze_feature_importance()\n",
    "predictor.get_segment_insights('Recreatief NL')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save Visitors Demand Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":    \n",
    "    try:\n",
    "        os.makedirs(\"../Data_Sources/Data_Cleaned/Predictions\", exist_ok=True)\n",
    "        \n",
    "        path = \"../Data_Sources/Data_Cleaned/Predictions/Segmented_Visitor_Demand_Prediction.csv\"\n",
    "        predictions_df.to_csv(path, index=False)\n",
    "        print(f\"Predictions saved successfully to: {path}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error saving file: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
